{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# <span style=\"color:blue\"> Face Recognition Deep Learning with PyWren over IBM Cloud Functions</span>"}, {"metadata": {}, "cell_type": "markdown", "source": "This notebook contains steps and code to demonstrate how serverless computing can provide great benefit for AI data preprocessing. We demonstrate face recognition using deep learning over the Watson Machine Learning service, while letting IBM Cloud Functions do the data preparation phase. As we will show this makes an entire process up to 50 times faster comparing to running the same code without leveraging serverless computing.\n\nOur notebook is based on a blog <a href=\"https://hackernoon.com/building-a-facial-recognition-pipeline-with-deep-learning-in-tensorflow-66e7645015b8\" target=\"_blank\" rel=\"noopener no referrer\">Building a Facial Recognition Pipeline with Deep Learning in Tensorflow</a> written by Cole Murray who kindly allowed us to use code and text from his blog.\n\nThis notebook introduces commands for interacting with your Watson Machine Learning service such as uploading training definitions and kicking off a training session.\n\nSome familiarity with Python is helpful. This notebook uses:\n\n- Python 3 \n- <a href=\"https://dataplatform.cloud.ibm.com/docs/content/analyze-data/environments-parent.html\" target=\"_blank\" rel=\"noopener no referrer\">Watson Studio environments.</a>\n- <a href=\"https://cloud.ibm.com/openwhisk\" target=\"_blank\" rel=\"noopener no referrer\">IBM Cloud Functions</a>\n- <a href=\"https://github.com/pywren/pywren-ibm-cloud\" target=\"_blank\" rel=\"noopener no referrer\">PyWren for IBM Cloud</a>\n\n\n\n## Learning goals\n\nIn this notebook, you will learn:\n\n-  How IBM Cloud Functions can be used for the data preparation phase\n-  The value of PyWren for IBM Cloud\n-  How to work with Watson Machine Learning to train Deep Learning models (TensorFlow + scikit-learn)\n-  How to retrieve and use models trained in Watson Machine Learning\n\n## Contents\n\n1. [Set up related IBM Cloud Services](#setup)\n2. [Dependencies installation](#dependencies-install)\n3. [Configuration](#configuration)\n4. [Preprocessing Data using Dlib, PyWren, and IBM Cloud Functions](#preprocessing)\n5. [Setup for WML](#wml-setup)\n6. [Create the training definitions](#training-definitions)\n7. [Train the model](#train)\n8. [Work with the Trained Model](#work)\n9. [Summary](#summary)\n"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"setup\"></a>\n## <span style=\"color:blue\">1. Set up related IBM Cloud Services</span>\n\nBefore you use the sample code in this notebook, you must setup Watson Machine Learning Service, IBM Cloud Object Storage and IBM Cloud Functions.\n"}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.1 Create Watson Machine Learning Service\n\nCreate a <a href=\"https://console.ng.bluemix.net/catalog/services/ibm-watson-machine-learning/\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a free plan is offered and information about how to create the instance is <a href=\"https://dataplatform.ibm.com/docs/content/analyze-data/wml-setup.html\" target=\"_blank\" rel=\"noopener no referrer\">here</a>)."}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.2 Create IBM Cloud Object Storage\n\nCreate a <a href=\"https://console.bluemix.net/catalog/infrastructure/cloud-object-storage\" target=\"_blank\" rel=\"noopener no referrer\">Cloud Object Storage (COS)</a> instance (a lite plan is offered and information about how to order storage is <a href=\"https://console.bluemix.net/docs/services/cloud-object-storage/basics/order-storage.html#order-storage\" target=\"_blank\" rel=\"noopener no referrer\">here</a>). <br/>**Note: When using Watson Studio, you already have a COS instance associated with the project you are running the notebook in.**\n\n- Create new credentials with HMAC: \n    - Go to your COS dashboard.\n    - In the **Service credentials** tab, click **New Credential+**.\n    - Add the inline configuration parameter: {\"HMAC\":true}, click **Add**. (For more information, see <a href=\"https://console.bluemix.net/docs/services/cloud-object-storage/hmac/credentials.html#using-hmac-credentials\" target=\"_blank\" rel=\"noopener no referrer\">HMAC</a>.)\n\n    This configuration parameter adds the following section to the instance credentials, (for use later in this notebook):\n    ```\n      \"cos_hmac_keys\": {\n            \"access_key_id\": \"-------\",\n            \"secret_access_key\": \"-------\"\n       }\n    ```\n\n"}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.3 Create IBM Cloud Functions account\nLog in to the <a href=\"https://cloud.ibm.com/openwhisk\" target=\"_blank\" rel=\"noopener no referrer\">Cloud Functions dashboard</a> and take note of where the `API Key` option is under `Getting Started` on the menu on the left. \n\nIf you want to ensure that your service is runnning, you can try quickly deploying the \"<a href=\"https://cloud.ibm.com/openwhisk/create/template/hello-world\" target=\"_blank\" rel=\"noopener no referrer\">Hello World</a>\" example under `Start Creating` > `Quickstart Templates` > `Hello World`."}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"dependencies-install\"></a>\n## <span style=\"color:blue\"> 2. Dependencies installation </span>"}, {"metadata": {}, "cell_type": "markdown", "source": "Install the needed libraries for the face recognition preprocessing.\nThe \"dlib\" dependency needs to be installed via new environment. Create a new environment based on Python 3.5 and add the dependency in the customization section as follows:\n\n    channels:\n    - conda-forge\n    dependencies:\n    - dlib\n\nInstructions for creating customized environments can be found in the code pattern <a href=\"https://github.com/IBM/data-pre-processing-with-pywren#5-create-a-custom-runtime-environment\" target=\"_blank\" rel=\"noopener no referrer\">README</a>."}, {"metadata": {}, "cell_type": "code", "source": "print(cv2.__version__)", "execution_count": 17, "outputs": [{"output_type": "stream", "text": "4.1.2\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "%%capture\ntry:\n    import cv2\nexcept:\n    !pip install --user opencv-contrib-python\ntry:\n    import openface\nexcept:    \n    !git clone https://github.com/cmusatyalab/openface.git\n    !pip install -r requirements.txt\nfrom uuid import uuid4\nimport bz2", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Install PyWren-IBM\nimport sys\nsys.executable, sys.prefix\n!{sys.executable} -m pip install -U pywren-ibm-cloud==1.0.9\nshutil.move('openface', '/opt/conda/envs/Python36/lib/python3.6/site-packages')\n!cd openface ; python setup.py install", "execution_count": 3, "outputs": [{"output_type": "stream", "text": "Collecting pywren-ibm-cloud==1.0.9\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/da/e10b6ec4a7b8d65443ed504c0ab74406b86b6584f06539002007adc15ce6/pywren_ibm_cloud-1.0.9-py3-none-any.whl (112kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 122kB 8.0MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: glob2 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pywren-ibm-cloud==1.0.9) (0.6)\nRequirement already satisfied, skipping upgrade: PyYAML in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pywren-ibm-cloud==1.0.9) (3.13)\nRequirement already satisfied, skipping upgrade: requests in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pywren-ibm-cloud==1.0.9) (2.21.0)\nRequirement already satisfied, skipping upgrade: Click in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pywren-ibm-cloud==1.0.9) (7.0)\nRequirement already satisfied, skipping upgrade: lxml in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pywren-ibm-cloud==1.0.9) (4.3.1)\nRequirement already satisfied, skipping upgrade: ibm-cos-sdk in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pywren-ibm-cloud==1.0.9) (2.4.3)\nRequirement already satisfied, skipping upgrade: tqdm in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pywren-ibm-cloud==1.0.9) (4.31.1)\nCollecting pika==0.13.1 (from pywren-ibm-cloud==1.0.9)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/cd/452c69f5963ef6dbbe4286e774f3e2e3eae10efd0bb9816c886ebdef3070/pika-0.13.1-py2.py3-none-any.whl (109kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112kB 10.1MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: matplotlib in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pywren-ibm-cloud==1.0.9) (3.0.2)\nRequirement already satisfied, skipping upgrade: seaborn in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pywren-ibm-cloud==1.0.9) (0.9.0)\nCollecting enum34 (from pywren-ibm-cloud==1.0.9)\n  Downloading https://files.pythonhosted.org/packages/af/42/cb9355df32c69b553e72a2e28daee25d1611d2c0d9c272aa1d34204205b2/enum34-1.1.6-py3-none-any.whl\nRequirement already satisfied, skipping upgrade: pandas in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pywren-ibm-cloud==1.0.9) (0.24.1)\nRequirement already satisfied, skipping upgrade: tblib in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pywren-ibm-cloud==1.0.9) (1.3.2)\nRequirement already satisfied, skipping upgrade: python-dateutil in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pywren-ibm-cloud==1.0.9) (2.7.5)\nRequirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests->pywren-ibm-cloud==1.0.9) (3.0.4)\nRequirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests->pywren-ibm-cloud==1.0.9) (1.24.1)\nRequirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests->pywren-ibm-cloud==1.0.9) (2.8)\nRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests->pywren-ibm-cloud==1.0.9) (2019.11.28)\nRequirement already satisfied, skipping upgrade: ibm-cos-sdk-s3transfer==2.*,>=2.0.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm-cos-sdk->pywren-ibm-cloud==1.0.9) (2.4.3)\nRequirement already satisfied, skipping upgrade: ibm-cos-sdk-core==2.*,>=2.0.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm-cos-sdk->pywren-ibm-cloud==1.0.9) (2.4.3)\nRequirement already satisfied, skipping upgrade: numpy>=1.10.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from matplotlib->pywren-ibm-cloud==1.0.9) (1.17.4)\nRequirement already satisfied, skipping upgrade: cycler>=0.10 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from matplotlib->pywren-ibm-cloud==1.0.9) (0.10.0)\nRequirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from matplotlib->pywren-ibm-cloud==1.0.9) (1.0.1)\nRequirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from matplotlib->pywren-ibm-cloud==1.0.9) (2.3.1)\nRequirement already satisfied, skipping upgrade: scipy>=0.14.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from seaborn->pywren-ibm-cloud==1.0.9) (1.3.2)\nRequirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas->pywren-ibm-cloud==1.0.9) (2018.9)\nRequirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from python-dateutil->pywren-ibm-cloud==1.0.9) (1.12.0)\nRequirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk->pywren-ibm-cloud==1.0.9) (0.9.3)\nRequirement already satisfied, skipping upgrade: docutils>=0.10 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk->pywren-ibm-cloud==1.0.9) (0.14)\nRequirement already satisfied, skipping upgrade: setuptools in /opt/conda/envs/Python36/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->pywren-ibm-cloud==1.0.9) (40.8.0)\nInstalling collected packages: pika, enum34, pywren-ibm-cloud\nSuccessfully installed enum34-1.1.6 pika-0.13.1 pywren-ibm-cloud-1.0.9\n/usr/bin/sh: line 0: cd: openface: No such file or directory\npython: can't open file 'setup.py': [Errno 2] No such file or directory\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"configuration\"></a>\n## <span style=\"color:blue\">3. Configuration </span>\nThis section explains how to configure the needed services."}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.1 Setup a bucket in IBM Cloud Object Storage\n\nYou need an IBM COS bucket which you will use to store the input data. If you don't know of any of your existing buckets or would like like to create a new one, please navigate to your <a href=\"https://cloud.ibm.com/resources\" target=\"_blank\" rel=\"noopener no referrer\">cloud resource list</a>,  then find and select your storage instance. From here, you will be able to view all your buckets and can create a new bucket in the region you prefer. Make sure you copy the correct endpoint for the bucket from the `Endpoint` tab of this COS service dashboard.\n\n**Note:** The bucket names must be unique."}, {"metadata": {}, "cell_type": "code", "source": "# Fill here the bucket name you created in COS Dashboard \nBUCKET = 'cloudbuttonbucket01'", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Define COS endpoint information. Example of US Cross Region endpoint\ncos_endpoint = 'https://s3.eu.cloud-object-storage.appdomain.cloud'", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.2 COS Connection\nNow connect to the Cloud Object Storage service by first obtaining your credentials.\n\nYou can find COS credentials in your COS instance dashboard under the `Service credentials` tab.\nNote: the HMAC key, described in set up the environment is included in these credentials.\n\n"}, {"metadata": {}, "cell_type": "code", "source": "#cos_credentials = {\n # \"apikey\": \"***\",\n  #\"cos_hmac_keys\": {\n   # \"access_key_id\": \"***\",\n    #\"secret_access_key\": \"***\"\n  #},\n#  \"endpoints\": \"https://cos-service.bluemix.net/endpoints\",\n # \"iam_apikey_description\": \"Auto generated apikey during resource-key operation for Instance - crn:v1:bluemix:public:cloud-object-storage:global:a/07a95aa44e6124e8b320b70cf88033fa:876e5285-4bef-4cf3-a89b-595e19648c7c::\",\n  #\"iam_apikey_name\": \"auto-generated-apikey-19a79dae-6a58-4b4f-878f-6839b711523f\",\n  #\"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",\n  #\"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/07a95aa44e6124e8b320b70cf88033fa::serviceid:ServiceId-3f2cccee-61ec-4147-8732-9f58479ba26a\",\n  #\"resource_instance_id\": \"crn:v1:bluemix:public:cloud-object-storage:global:a/07a95aa44e6124e8b320b70cf88033fa:876e5285-4bef-4cf3-a89b-595e19648c7c::\"\n#}\n\ncos_credentials = {\n  \"apikey\": \"RRJtoAasvR3PJtezkSNWL5HLowZ5D3s9FDINBlxIBosV\",\n  \"cos_hmac_keys\": {\n    \"access_key_id\": \"26e07104896d48f3b4fd590cc438bd72\",\n    \"secret_access_key\": \"e01b1f7b47a3eacf511bb7a647eeb4a6fc338b84757a12dd\"\n  },\n  \"endpoints\": \"https://control.cloud-object-storage.cloud.ibm.com/v2/endpoints\",\n  \"iam_apikey_description\": \"Auto-generated for key 26e07104-896d-48f3-b4fd-590cc438bd72\",\n  \"iam_apikey_name\": \"cloudbutton-creds\",\n  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",\n  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/6db93b0269a945088943593588f6fd69::serviceid:ServiceId-08ed0ac7-65e8-41b5-9a1d-89f3075a2ffb\",\n  \"resource_instance_id\": \"crn:v1:bluemix:public:cloud-object-storage:global:a/6db93b0269a945088943593588f6fd69:007c3566-78ae-4018-8e2c-58411b04f488::\"\n}", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "You also need the IBM Cloud authorization endpoint to be able to create COS resource object."}, {"metadata": {}, "cell_type": "code", "source": "# Define the authorization endpoint.\nauth_endpoint = 'https://iam.bluemix.net/oidc/token'", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Install the boto library if necessary. This library allows Python developers to manage IBM Cloud Object Storage (COS). However, most environments on Watson Studio have this preinstalled."}, {"metadata": {}, "cell_type": "markdown", "source": "**Tip:** If `ibm_boto3` is not preinstalled in you environment, run the following command to install it: "}, {"metadata": {}, "cell_type": "code", "source": "# Uncomment and run the following command if ibm_boto3 is not installed.\n# !pip install ibm-cos-sdk", "execution_count": 13, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Import the boto library.\nimport ibm_boto3\nfrom ibm_botocore.client import Config", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Create a Boto resource to be able to write data to COS."}, {"metadata": {}, "cell_type": "code", "source": "# Create a COS resource.\ncos = ibm_boto3.resource('s3',\n                         ibm_api_key_id=cos_credentials['apikey'],\n                         ibm_service_instance_id=cos_credentials['resource_instance_id'],\n                         ibm_auth_endpoint=auth_endpoint,\n                         config=Config(signature_version='oauth'),\n                         endpoint_url=cos_endpoint)", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.3 Verify you can access your COS Bucket\n\nIf you fail to access your bucket, make sure you use the correct bucket name and endpoint URL for the region where the bucket was created. If you see no errors after running the following cell, you are good to go."}, {"metadata": {}, "cell_type": "code", "source": "try: BUCKET\nexcept NameError: BUCKET = None\n\nif not BUCKET:\n    print (\"Error. Bucket can not be empty. Please create bucket in COS Dashboard UI and update 'BUCKET'\")\n\ntry: cos\nexcept NameError: cos = None\n\nif not cos:\n    print(\"Error. Please create ibm_boto3 instance\")\n\nif cos and not cos.Bucket(BUCKET) in cos.buckets.all():\n    print (\"Error. Bucket not found. Please make sure cos_endpoint targets the region of the bucket\")", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.4 IBM Cloud Functions setup"}, {"metadata": {}, "cell_type": "markdown", "source": "Obtain the API key and endpoint to the <a href=\"https://cloud.ibm.com/openwhisk\" target=\"_blank\" rel=\"noopener no referrer\">IBM Cloud Functions service</a>. Navigate to `Getting Started` > `API Key` from the side menu and copy the values for \"Current Namespace\", \"Host\" and \"Key\" into the config below. Make sure to add \"https://\" to the host when adding it as the endpoint."}, {"metadata": {}, "cell_type": "code", "source": "config = {\n    'ibm_cf':  {'endpoint': 'https://eu-gb.functions.cloud.ibm.com', \n                      'namespace': 'cloudbutton@uvigo.es_dev', \n                      'runtime_memory' : 256,\n                      'api_key': 'c8a9e3ec-51c9-413b-ac23-e10c3ccb71e1:k3GoGB7GRgyNFYI3ob97GAuKt8ORPYJ9eWjCfIygD0d2xeR9aowjaQlvgm7HhlPm'}, \n    'ibm_cos': {\n        'endpoint': cos_endpoint,\n        'api_key' :  cos_credentials['apikey']\n    },\n    'pywren' : {'storage_bucket' : BUCKET,}\n                #'runtime' : 'ibmfunctions/pywren-dlib-runtime:3.6'}\n}\n", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The PyWren engine requires its server side component to be deployed in advance. This step creates a new IBM Cloud Functions function with the PyWren server side runtime. This action will be used internally by PyWren during execution phases."}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"preprocessing\"></a>\n## <span style=\"color:blue\">4. Preprocessing Data using Dlib, PyWren, and IBM Cloud Functions</span>"}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.1 Upload input data into IBM Cloud Object Storage\n\nYour COS Bucket should contain the raw dataset of images with the following structure:\n\n     Directory Structure\n     \u251c\u2500\u2500 Tyra_Banks\n     \u2502 \u251c\u2500\u2500 Tyra_Banks_0001.jpg\n     \u2502 \u2514\u2500\u2500 Tyra_Banks_0002.jpg\n     \u251c\u2500\u2500 Tyron_Garner\n     \u2502 \u251c\u2500\u2500 Tyron_Garner_0001.jpg\n     \u2502 \u2514\u2500\u2500 Tyron_Garner_0002.jpg\n     \nIf you don't have any images, we will demonstrate how to use the [LFW](http://vis-www.cs.umass.edu/lfw/) (Labeled Faces in the Wild) dataset as training data. Below are instructions how you can upload this dataset into your private COS bucket.\n\n**You should run this only once. If images were already created in any previous run, you can skip this section.**"}, {"metadata": {}, "cell_type": "markdown", "source": "The following step copies images from Labeled Faces in the Wild into your COS bucket.\nWe demonstrate with small data set of about 14MB. If you wish to use the entire data set, then use:\n\n    url = \"http://vis-www.cs.umass.edu/lfw/lfw.tgz\""}, {"metadata": {}, "cell_type": "code", "source": "import urllib.request\nimport tarfile\nimport io\nfrom multiprocessing import Pool, Value, Lock\n\ncounter = Value(\"i\", 0)\ncounter_lock = Lock()\n\ndef increment():\n    with counter_lock:\n        counter.value += 1\n        if counter.value % 100 == 0:\n            print(\"%d Images uploaded...\" % counter.value)\n\ndef copy(target_key, content):        \n    cos.Object(BUCKET, target_key).put(Body=content)\n    increment()\n\ndef extractFromStream(url, cos, target_prefix = None):\n    procs = []\n\n    pool = Pool(processes=16)\n    ftp_stream = urllib.request.urlopen(url)\n    tarfile_like_object = io.BytesIO(ftp_stream.read())\n    TarFile_object = tarfile.open(fileobj=tarfile_like_object)\n    for member in TarFile_object:\n        if member.isdir() == False:\n            member_like_object = TarFile_object.extractfile(member)\n            # member.name has prefix 'lfw' and is of the form:\n            # lfw/name_familyname/name_familyname_0001.jpg\n            # The following code removes prefix lfw from the name.\n            candidate_key = member.name\n            if candidate_key.find(\"/\") >= 0:\n                ind = candidate_key.index(\"/\") + 1\n                candidate_key = candidate_key[ind:]\n\n            key = candidate_key\n            if target_prefix is not None:\n                key = target_prefix + '/' + candidate_key\n            res = pool.apply_async(copy, args=(key, member_like_object.read()))\n\n    pool.close()\n    pool.join()\n    print(\"%d Images uploaded...\" % counter.value)\n\n\nurl = \"http://vis-www.cs.umass.edu/lfw/lfw-a.tgz\"\nextractFromStream(url, cos, \"images\")", "execution_count": 12, "outputs": [{"output_type": "stream", "text": "100 Images uploaded...\n200 Images uploaded...\n300 Images uploaded...\n400 Images uploaded...\n500 Images uploaded...\n600 Images uploaded...\n700 Images uploaded...\n800 Images uploaded...\n900 Images uploaded...\n1000 Images uploaded...\n1054 Images uploaded...\n", "name": "stdout"}]}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "bucket = cos.Bucket(BUCKET)\nprint(bucket.name + \" contains \" + str(len(list(bucket.objects.filter(Prefix='images')))) + \" images\")", "execution_count": 13, "outputs": [{"output_type": "stream", "text": "cloudbuttonbucket01 contains 1054 images\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.2 Data preprocessing with serverless\n\nBelow, you\u2019ll preprocess the images before passing them into the FaceNet model. Image pre-processing in a facial recognition context typically solves a few problems. These problems range from lighting differences, occlusion, alignment, and segmentation. Below, you\u2019ll address segmentation and alignment.\n\nFirst, you\u2019ll solve the segmentation problem by finding the largest face in an image. This is useful as our training data does not have to be cropped for a face ahead of time.\n\nSecond, you\u2019ll solve alignment. In photographs, it is common for a face to not be perfectly center aligned with the image. To standardize input, you\u2019ll apply a transform to center all images based on the location of eyes and bottom lip.\n"}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.3 Detect, Crop & Align with Dlib\n\nUpload dlib\u2019s face landmark predictor into your COS bucket. You\u2019ll use this face landmark predictor to find the location of the inner eyes and bottom lips of a face in an image. These coordinates will be used to center align the image.\n\n**You should run this only once. If the predictor was already created in a previous run, you can skip this section.**\n"}, {"metadata": {}, "cell_type": "code", "source": "import urllib.request\nimport io\n\ndef uploadFileFromStream(url, name, cos, target_prefix = None):\n    ftp_stream = urllib.request.urlopen(url + name)\n    file_like_object = io.BytesIO(ftp_stream.read())\n    src = bz2.BZ2File(file_like_object, \"rb\")\n    key = name[:-4]\n    if target_prefix is not None:\n        key = target_prefix + '/' + name[:-4]\n    cos.Object(BUCKET, key).put(Body=src.read())\n\nuploadFileFromStream(\"http://dlib.net/files/\", \"shape_predictor_68_face_landmarks.dat.bz2\", cos, 'predictor')", "execution_count": 14, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.4 Preprocessing with IBM Cloud Functions\nNext, you\u2019ll create a preprocessor for your dataset. This file will read each image into memory, attempt to find the largest face, center align, and write the file to output. If a face cannot be found in the image, it will be skipped.\nAs each image can be processed independently, PyWren will be used with IBM Cloud Functions to scale out the processing."}, {"metadata": {}, "cell_type": "code", "source": "import logging\nimport os\nimport time\nimport pywren_ibm_cloud as pywren\nimport shutil\nimport cv2\n\n#from openface.align_dlib import AlignDlib\nlogger = logging.getLogger(__name__)\n\ntemp_dir = '/tmp'\n\ndef preprocess_image(bucket, key, data_stream, ibm_cos):\n    \"\"\"\n    Detect face, align and crop :param input_path. Write output to :param output_path\n    :param bucket: COS bucket\n    :param key: COS key (object name ) - may contain delimiters\n    :param storage_handler: can be used to read / write data from / into COS\n    \"\"\"\n    crop_dim = 180\n    print(\"Process bucket {} key {}\".format(bucket, key))    \n    # key of the form /subdir1/../subdirN/file_name\n    key_components = key.split('/')\n    file_name = key_components[len(key_components)-1]\n    input_path = temp_dir + '/' + file_name\n    if not os.path.exists(temp_dir + '/' + 'output'):\n        os.makedirs(temp_dir + '/' +'output')\n    output_path = temp_dir + '/' +'output/'  + file_name\n    with open(input_path, 'wb') as localfile:\n        shutil.copyfileobj(data_stream, localfile)\n    exists = os.path.isfile(temp_dir + '/' +'shape_predictor_68_face_landmarks')\n    if exists:\n        pass;\n    else:\n        res = ibm_cos.get_object(Bucket = bucket, Key = 'predictor/shape_predictor_68_face_landmarks.dat')\n        with open(temp_dir + '/' +'shape_predictor_68_face_landmarks', 'wb') as localfile:\n            shutil.copyfileobj(res['Body'], localfile)\n    openface.align_dlib = openface.align_dlib.AlignDlib(temp_dir + '/' +'shape_predictor_68_face_landmarks')\n    image = _process_image(input_path, crop_dim, align_dlib)\n    if image is not None:\n        print('Writing processed file: {}'.format(output_path))\n        cv2.imwrite(output_path, image)\n        f = open(output_path, \"rb\")\n        processed_image_path = os.path.join('output',key)\n        ibm_cos.put_object(Bucket = bucket, Key = processed_image_path, Body = f)\n        os.remove(output_path)\n    else:\n        print(\"Skipping filename: {}\".format(input_path))\n    os.remove(input_path)\n\ndef _process_image(filename, crop_dim, align_dlib):\n    image = None\n    aligned_image = None\n    image = _buffer_image(filename)\n    if image is not None:\n        aligned_image = _align_image(image, crop_dim, align_dlib)\n    else:\n        raise IOError('Error buffering image: {}'.format(filename))\n    return aligned_image\n\ndef _buffer_image(filename):\n    logger.debug('Reading image: {}'.format(filename))\n    image = cv2.imread(filename, )\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return image\n\ndef _align_image(image, crop_dim, align_dlib):\n    bb = align_dlib.getLargestFaceBoundingBox(image)\n    aligned = align_dlib.align(crop_dim, image, bb, landmarkIndices=openface.align_dlib.AlignDlib.INNER_EYES_AND_BOTTOM_LIP)\n    if aligned is not None:\n        aligned = cv2.cvtColor(aligned, cv2.COLOR_BGR2RGB)\n    return aligned", "execution_count": 15, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.5 Getting Results\n\nNow that you\u2019ve created a pipeline, time to get results. In the following cell, an executor is created using the configuration and runtime that was set up above. Then the `map` function is used to launch the preprocessing function with all of images located in your IBM Cloud Object Storage bucket. This makes it so that the image preprocessing is massively parallel using IBM Cloud Functions."}, {"metadata": {}, "cell_type": "code", "source": "raw_images = BUCKET + '/images'\npw = pywren.ibm_cf_executor(config=config)\ntry:\n    pw.map(preprocess_image, raw_images, remote_invocation=True)\n    results = pw.get_result()\n    print(\"Execution completed\")\nexcept Exception as e:\n    print (e)\nfinally:    \n    pw.clean()", "execution_count": 16, "outputs": [{"output_type": "stream", "text": "IBM Cloud Functions init for Namespace: cloudbutton@uvigo.es_dev\nIBM Cloud Functions init for Host: https://eu-gb.functions.cloud.ibm.com\nIBM Cloud Functions init for Runtime: ibmfunctions/action-python-v3.6 - 256MB \nIBM Cloud Functions executor created with ID 2cfd4cfb-b2f6\nExecutor ID 2cfd4cfb-b2f6 Uploading function and data - Total: 3.1KiB\nExecutor ID 2cfd4cfb-b2f6 Starting function invocation: preprocess_image() - Total: 1 activations\nExecutor ID 2cfd4cfb-b2f6 Getting results...\nNo module named 'cv2'\nExecutor ID 2cfd4cfb-b2f6 Cleaning partial results from cos://cloudbuttonbucket01/pywren.jobs/2cfd4cfb-b2f6\n\n", "name": "stdout"}]}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "%tb", "execution_count": 44, "outputs": [{"output_type": "error", "ename": "NameError", "evalue": "name 'openface' is not defined", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m<ipython-input-39-721dc8260d14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mopenface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign_dlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlignDlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopenface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;31mNameError\u001b[0m: name 'openface' is not defined"]}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Review\n\nUsing Dlib, you detected the largest face in an image and aligned the center of the face by the inner eyes and bottom lip. This alignment is a method for standardizing each image for use as feature input.\n\nVerify that images were processed with dlib:"}, {"metadata": {}, "cell_type": "code", "source": "bucket = cos.Bucket(BUCKET)\nprint(str(len(list(bucket.objects.filter(Prefix='output/images')))) + \" images were pre-processed with dlib\")\nprint (\"COS location of pre-processed images is: \" + bucket.name + '/output/images/')", "execution_count": 29, "outputs": [{"output_type": "stream", "text": "0 images were pre-processed with dlib\nCOS location of pre-processed images is: cloudbuttonbucket01/output/images/\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"wml-setup\"></a>\n## <span style=\"color:blue\">5. Setup for WML</span>\n\nNow that we've preprocessed the data, we\u2019ll generate vector embeddings of each identity. These embeddings can then be used as input to a classification, regression, or clustering task. We will use TensorFlow to create the embeddings and then scikit-learn to create the classifier with these embeddings. However, before we do all this, some preliminary setup is needed.\n\nIn this section we:\n\n- [5.1 Download Pretrained Model](#download-model)\n- [5.2 Save model files to Cloud Object Storage](#save-model-cos)\n- [5.3 Authenticate with the WML service instance](#wml-service-instance)"}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.1 Download pretrained model<a id=\"download-model\"></a>\n\nWe will use a pretrained model to simplify the process of generating the embeddings. The model we will use is based off the Inception ResNet V1 architecture and was trained using the <a href=\"http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/\" target=\"_blank\" rel=\"noopener no referrer\">VGGFace2</a> dataset. With the pretrained model and its learned weights, we can use transfer learning in order to create a model that can classify the LFW faces.\n\nFirst, we download and extract the pretrained model using a script copied from <a href=\"https://github.com/davidsandberg/facenet/blob/master/src/download_and_extract.py\" target=\"_blank\" rel=\"noopener no referrer\">here</a>: "}, {"metadata": {}, "cell_type": "code", "source": "import requests\nimport zipfile\nimport os\n\n\"\"\"\nThis file is copied and adapted from:\nhttps://github.com/davidsandberg/facenet/blob/master/src/download_and_extract.py\n\"\"\"\n\nmodel_dict = {\n    '20180402-114759': '1EXPBSXwTaqrSC0OhUdXNmKSh9qJUQ55-'\n}\n\ndef download_and_extract_file(model_name, data_dir):\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    file_id = model_dict[model_name]\n    destination = os.path.join(data_dir, model_name + '.zip')\n    if not os.path.exists(destination):\n        print('Downloading file to %s' % destination)\n        download_file_from_google_drive(file_id, destination)\n        with zipfile.ZipFile(destination, 'r') as zip_ref:\n            print('Extracting file to %s' % data_dir)\n            zip_ref.extractall(data_dir)\n\ndef download_file_from_google_drive(file_id, destination):\n\n        URL = \"https://drive.google.com/uc?export=download\"\n        session = requests.Session()\n        response = session.get(URL, params = { 'id' : file_id }, stream = True)\n        token = get_confirm_token(response)\n\n        if token:\n            params = { 'id' : file_id, 'confirm' : token }\n            response = session.get(URL, params = params, stream = True)\n\n        save_response_content(response, destination)\n\ndef get_confirm_token(response):\n    for key, value in response.cookies.items():\n        if key.startswith('download_warning'):\n            return value\n    return None\n\ndef save_response_content(response, destination):\n    CHUNK_SIZE = 32768\n\n    with open(destination, \"wb\") as f:\n        for chunk in response.iter_content(CHUNK_SIZE):\n            if chunk: # filter out keep-alive new chunks\n                f.write(chunk)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Initiate the download\n\nThe download is about 184Mb in size."}, {"metadata": {}, "cell_type": "code", "source": "download_and_extract_file('20180402-114759', './pretrained-model')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.2 Save model files to Cloud Object Storage <a id=\"save-model-cos\"></a>"}, {"metadata": {}, "cell_type": "code", "source": "# If you have another bucket you want to save the model files to, set it here,\n# otherwise we will use the same bucket.\n\n# BUCKET = ''\nbucket_obj = cos.Bucket(BUCKET)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import glob\nimport os\n\nfiles_search = os.path.join('./pretrained-model/*/*')\nfiles = glob.glob(files_search)\n\n\nfor file in files:\n    filename = file.split('/')[-1]\n    filename = os.path.join(\"pretrained-model\", filename)\n    print('Uploading data {}...'.format(filename))\n    bucket_obj.upload_file(file, filename )\n    print('{} is uploaded.'.format(filename))\nprint(\"Done\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.3. Authenticate with the WML service instance <a id=\"wml-service-instance\"></a>\n\nImport the libraries you need to work with your WML instance.\n\n**Hint**: You may also need to install `wget` using the following command `!pip install wget`"}, {"metadata": {}, "cell_type": "code", "source": "!pip install wget", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import urllib3, requests, json, base64, time, os, wget", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Authenticate to the Watson Machine Learning (WML) service on IBM Cloud.\n\n**Tip**: Authentication information (your credentials) can be found in the <a href=\"https://console.bluemix.net/docs/services/service_credentials.html#service_credentials\" target=\"_blank\" rel=\"noopener noreferrer\">Service credentials</a> tab of the service instance that you created on IBM Cloud. \nIf there are no credentials listed for your instance in **Service credentials**, click **New credential (+)** and enter the information required to generate new authentication information. \n\n**Action**: Enter your WML service instance credentials here.\n\n`\nwml_credentials = {\n  \"apikey\": \"------\",\n  \"iam_apikey_description\": \"------:\",\n  \"iam_apikey_name\": \"------\",\n  \"iam_role_crn\": \"-------\",\n  \"iam_serviceid_crn\": \"-------\",\n  \"instance_id\": \"-------\",\n  \"password\": \"------\",\n  \"url\": \"------\",\n  \"username\": \"-------\"\n}\n`"}, {"metadata": {}, "cell_type": "code", "source": "# @hidden_cell\nwml_credentials = {\n\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Import the `watson-machine-learning-client` and authenticate to the service instance.\n\n**Tip:** If `watson-machine-learning-client` is not preinstalled in your environment, run the following command to install it: "}, {"metadata": {}, "cell_type": "code", "source": "!pip install watson-machine-learning-client", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**Note:** A deprecation warning may be returned from scikit-learn package that does not impact watson machine learning client functionalities."}, {"metadata": {}, "cell_type": "code", "source": "client = WatsonMachineLearningAPIClient(wml_credentials)\nprint(client.version)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"training-definitions\"></a>\n## <span style=\"color:blue\">6. Create the training definitions</span>\n\nWith us now connected to our WML service instance, we can now create the training definitions.\n\nIn this section you:\n\n- [6.1 Prepare the training definition metadata](#prep)\n- [6.2 Get the sample model definition content files from Git](#get)\n- [6.3 Store the training definition in the WML repository](#store)\n\n**Note:** `watson-machine-learning-client` documentation can be found <a href=\"http://wml-api-pyclient.mybluemix.net/\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>."}, {"metadata": {}, "cell_type": "markdown", "source": "### 6.1 Prepare the training definition metadata<a id=\"prep\"></a>\n\nPrepare the training definition metadata. The main program will use the\nenviroment variables `$DATA_DIR` and `$RESULT_DIR` in the inputs for the\n`--model-path`, `--input-dir`, and `--output-path` options.\n\n**Tip:** You may want to change the number of epochs to be larger."}, {"metadata": {}, "cell_type": "code", "source": "training_definition_metadata = {\n    client.repository.DefinitionMetaNames.NAME: \"TensorFlow Facial Recognition\",\n    client.repository.DefinitionMetaNames.DESCRIPTION: \"Face Classifier\",\n    client.repository.DefinitionMetaNames.AUTHOR_NAME: \"IBM Developer\",\n    client.repository.DefinitionMetaNames.FRAMEWORK_NAME: \"tensorflow\",\n    client.repository.DefinitionMetaNames.FRAMEWORK_VERSION: \"1.11\",\n    client.repository.DefinitionMetaNames.RUNTIME_NAME: \"python\",\n    client.repository.DefinitionMetaNames.RUNTIME_VERSION: \"3.6\",\n    client.repository.DefinitionMetaNames.EXECUTION_COMMAND: \" \\\n        python3 train_classifier.py \\\n            --model-path $DATA_DIR/pretrained-model/20180402-114759.pb \\\n            --input-dir $DATA_DIR/output/images \\\n            --output-path $RESULT_DIR/output-classifier.pkl \\\n            --num-epochs 3\"\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 6.2 Get the sample model definition content file from GitHub <a id=\"get\"></a>"}, {"metadata": {}, "cell_type": "code", "source": "filename='tf-facial-recog.zip'\n\nif not os.path.isfile(filename):\n    filename = wget.download('https://github.com/IBM/data-pre-processing-with-pywren/raw/master/data/code/tf-facial-recog.zip')\n    print(filename, \"was downloaded\")\nelse:\n    print(filename, \"was downloaded previously.\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The files in this zip file can be viewed in the GitHub <a href=\"https://github.com/IBM/data-pre-processing-with-pywren/tree/master/data/code\" target=\"_blank\" rel=\"noopener noreferrer\">repository</a>."}, {"metadata": {}, "cell_type": "markdown", "source": "### 6.3 Store the training definition in the WML repository<a id=\"store\"></a>"}, {"metadata": {}, "cell_type": "code", "source": "definition_details = client.repository.store_definition(filename, training_definition_metadata)\ndefinition_uid = client.repository.get_definition_uid(definition_details)\n\n# Display the training definition uid.\nprint(definition_uid)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## <span style=\"color:blue\">7. Train the model</span><a id=\"train\"></a>\n\nIn this section, learn how to:\n- [7.1 Enter training configuration metadata](#meta)\n- [7.2 Train the model in the background](#backg)\n- [7.3 Monitor the training log](#log)"}, {"metadata": {}, "cell_type": "markdown", "source": "### 7.1 Enter training configuration metadata<a id=\"meta\"></a>\n\n- `TRAINING_DATA_REFERENCE` - references the uploaded training data.\n- `TRAINING_RESULTS_REFERENCE` - location where trained model will be saved.\n\nFor this exercise, we are going to use the same bucket as the input data to store our resulting model.\n\n**Note** Your COS credentials are referenced in this code.\n"}, {"metadata": {}, "cell_type": "code", "source": "# Configure the training metadata for the TRAINING_DATA_REFERENCE and TRAINING_RESULTS_REFERENCE.\ntraining_configuration_metadata = {\n    client.training.ConfigurationMetaNames.NAME: \"Face Classifier\", \n    client.training.ConfigurationMetaNames.AUTHOR_NAME: \"IBM Developer\",              \n    client.training.ConfigurationMetaNames.DESCRIPTION: \"Training for Face Classifier\",\n    client.training.ConfigurationMetaNames.COMPUTE_CONFIGURATION: {\"name\": \"k80\"},\n    client.training.ConfigurationMetaNames.TRAINING_DATA_REFERENCE: {\n        \"connection\": {\n            \"endpoint_url\": cos_endpoint,\n            \"access_key_id\": cos_credentials['cos_hmac_keys']['access_key_id'],\n            \"secret_access_key\": cos_credentials['cos_hmac_keys']['secret_access_key']\n        },\n        \"source\": {\n            \"bucket\": BUCKET,\n        },\n        \"type\": \"s3\"\n    },\n    client.training.ConfigurationMetaNames.TRAINING_RESULTS_REFERENCE: {\n        \"connection\": {\n            \"endpoint_url\": cos_endpoint,\n            \"access_key_id\": cos_credentials['cos_hmac_keys']['access_key_id'],\n            \"secret_access_key\": cos_credentials['cos_hmac_keys']['secret_access_key']\n        },\n        \"target\": {\n            \"bucket\": BUCKET,\n        },\n        \"type\": \"s3\"\n    },\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 7.2 Train the model in the background<a id=\"backg\"></a>\n\nTo run the training in the **background**, set the optional parameter `asynchronous=True` (or remove it). In this case the parameter has been removed. \n\n**Note:** To run the training in **active** mode, set `asynchronous=False`."}, {"metadata": {}, "cell_type": "code", "source": "# Start the training run.\ntraining_run_details = client.training.run(definition_uid, training_configuration_metadata)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Using the power of WML, the embedding creation and subsequent training should be relatively quick.\nWhile training, we will be applying additional random transformations and augmentations to the images, boosting our dataset. These images will be fed in a batch size of 128 into the model, and the model will return a 512 dimensional embedding for each image. After these embeddings are created, they will be used as feature inputs into a scikit-learn\u2019s SVM classifier to train on each class. Classes with less than 10 images will be dropped. This parameter is tunable in the execution command specified in the `training_definition_metadata` dictionary above.\n\nGet the training run GUID:"}, {"metadata": {}, "cell_type": "code", "source": "training_run_guid_async = client.training.get_run_uid(training_run_details)\nprint(\"training_run_guid_async=\",training_run_guid_async)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Check the status of the training run by calling the following method:"}, {"metadata": {}, "cell_type": "code", "source": "# Get training run status.\nstatus = client.training.get_status(training_run_guid_async)\nprint(json.dumps(status, indent=2))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 7.3  Monitor the training log<a id=\"log\"></a>\n\nRun the cell below to monitor the training log. This will continue monitoring until the run is finished. If you wish to stop monitoring a current training run, click on the stop button next to \"Run\" button at the top in the notebook options."}, {"metadata": {}, "cell_type": "code", "source": "client.training.monitor_logs(training_run_guid_async)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**Note**: You can cancel the training run by calling the method `client.training.cancel(training_run_guid_async)`\n\nAfter the training is complete, get the training GUID:"}, {"metadata": {}, "cell_type": "code", "source": "training_details = client.training.get_details(training_run_guid_async)\ntraining_guid = training_details[\"entity\"][\"training_results_reference\"][\"location\"][\"model_location\"]\nprint(\"Training GUID is:\", training_guid)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"work\"></a>\n## <span style=\"color:blue\">8. Work with the Trained Model</span>\n\nAfter the training is complete, the trained model is saved as a file named\n`output-classifier.pkl` in the result bucket.\nThe following code will fetch the model file from the bucket.\n\n**Tip:** Make sure that the training run is completed by checking its\nstatus as shown earlier."}, {"metadata": {}, "cell_type": "code", "source": "bucket_obj = cos.Bucket(BUCKET)\n\n# Trained model file name as defined in the code.\nsaved_model_filename = \"output-classifier.pkl\"\nsource_file = os.path.join(training_guid, saved_model_filename)\nbucket_obj.download_file(source_file,saved_model_filename)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!ls", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Use model to classify images\n\nWe are going to use parts of the code that we used with WML to classify images using our trained classifier. Typically we'd want to run the sample image through the same preprocessing steps we used on the training images (face alignment), however we get decent performance just resizing the input images. So let's set that up now:"}, {"metadata": {}, "cell_type": "code", "source": "import os\nimport pickle\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.platform import gfile\nfrom IPython.display import display\nfrom PIL import Image\n\n\n# Use the pretrained model we downloaded previously.\nmodel_path = './pretrained-model/20180402-114759/20180402-114759.pb'\n\n# The trained classifier we downloaded from our COS bucket after training.\nclassifier_path = 'output-classifier.pkl'\n\n\ndef run_model(image_paths):\n    with tf.Session(config=tf.ConfigProto(log_device_placement=False)) as sess:\n        _load_model(model_filepath=model_path)\n\n        dataset = tf.contrib.data.Dataset.from_tensor_slices((image_paths)) \\\n                .map(_preprocess_function) \\\n                .batch(128)\n\n        init_op = tf.group(tf.global_variables_initializer(),\n                           tf.local_variables_initializer())\n        sess.run(init_op)\n\n        iterator = dataset.make_one_shot_iterator()\n        batch = iterator.get_next()\n        batch_images = sess.run(batch)\n\n        images_placeholder = \\\n            tf.get_default_graph().get_tensor_by_name(\"input:0\")\n        embedding_layer = \\\n            tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n        phase_train_placeholder = \\\n            tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n\n        emb = sess.run(embedding_layer,\n                feed_dict={images_placeholder: batch_images,\n                           phase_train_placeholder: False})\n\n        with open(classifier_path, 'rb') as f:\n            model, class_names = pickle.load(f)\n\n            predictions = model.predict_proba(emb, )\n            for index, prediction in enumerate(predictions):\n                # Display the image in the notebook.\n                img = Image.open(image_paths[index])\n                img.thumbnail((100, 100))\n                display(img)\n\n                # Get the indices that would sort the array, then only get the\n                # indices that correspond to the top 3 predictions.\n                sorted_indices = prediction.argsort()[::-1][:3]\n                for index in sorted_indices:\n                    label = class_names[index]\n                    confidence = prediction[index]\n                    print('%s (confidence = %.5f)' % (label, confidence))\n                print('------------')\n\n\ndef _preprocess_function(image_path):\n    file_contents = tf.read_file(image_path)\n    image = tf.image.decode_jpeg(file_contents, channels=3)\n    image = tf.image.resize_images([image], (300, 300))[0]\n    image = tf.image.resize_image_with_crop_or_pad(image, 160, 160)\n    image = tf.image.per_image_standardization(image)\n    return image\n\n\ndef _load_model(model_filepath):\n    model_exp = os.path.expanduser(model_filepath)\n    if os.path.isfile(model_exp):\n        with gfile.FastGFile(model_exp, 'rb') as f:\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(f.read())\n            tf.import_graph_def(graph_def, name='')\n    else:\n        raise Exception('Specified model %s is not a file.' % (model_exp))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Download some sample images\n\nNow, we are going to download some sample images that we can run through the model, so feel free to add other URLs to the list below. Just make sure they are JPEG images. Since we aren't doing the facial alignment preprocessing for this step, pictures where the faces are more prominent work better."}, {"metadata": {}, "cell_type": "code", "source": "sample_images_dir = './sample-images'\nimage_urls = [\n    'https://upload.wikimedia.org/wikipedia/commons/b/b3/Adrien_Brody_Cannes_2017.jpg',\n    'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0c/SchwarzeneggerJan2010.jpg/800px-SchwarzeneggerJan2010.jpg'       \n]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import os\nimport wget\n\nif not os.path.exists(sample_images_dir):\n    os.makedirs(sample_images_dir)\n\nimages = []\nfor index, url in enumerate(image_urls):\n    outfile = os.path.join(sample_images_dir, 'sample{}.jpg'.format(index))\n    images.append(outfile)\n    wget.download(url, out=outfile)\n\nprint(images)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Run classifier on images\n\nNow that we downloaded some images, let's try classifying them:"}, {"metadata": {}, "cell_type": "code", "source": "run_model(images)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"summary\"></a>\n## <span style=\"color:blue\"> 9. Summary</span>"}, {"metadata": {}, "cell_type": "markdown", "source": "In this notebook, we used PyWren with IBM Cloud Functions to increase preprocessing performance and stored the resulting images in an IBM Cloud Object Storage bucket. From here, we used this bucket along with IBM Watson Machine Learning to create embeddings of each identity using a pretrained TensorFlow FaceNet model, and then created a custom face classifer. "}, {"metadata": {}, "cell_type": "markdown", "source": "### Citations\n\nQ. Cao, L. Shen, W. Xie, O. M. Parkhi, A. Zisserman. \"VGGFace2: A dataset for recognising face across pose and age\"  International Conference on Automatic Face and Gesture Recognition, 2018.\n\nGary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.\nLabeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments.\nUniversity of Massachusetts, Amherst, Technical Report 07-49, October, 2007."}, {"metadata": {}, "cell_type": "markdown", "source": "### References\n\n1. <a href=\"https://hackernoon.com/building-a-facial-recognition-pipeline-with-deep-learning-in-tensorflow-66e7645015b8\" target=\"_blank\" rel=\"noopener no referrer\">Building a Facial Recognition Pipeline with Deep Learning in Tensorflow</a> (Original inspiration for code pattern)\n2. <a href=\"https://github.com/davidsandberg/facenet\" target=\"_blank\" rel=\"noopener no referrer\">Face Recognition using Tensorflow</a>\n3. <a href=\"http://pywren.io/\" target=\"_blank\" rel=\"noopener no referrer\">PyWren</a>"}, {"metadata": {}, "cell_type": "markdown", "source": "### Authors\n\n**Gil Vernik**\n\n**Paul Van Eck**"}, {"metadata": {}, "cell_type": "markdown", "source": "Copyright \u00a9 2019 IBM. This notebook and its source code are released under the terms of the MIT License."}, {"metadata": {}, "cell_type": "markdown", "source": "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n</div>"}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.8", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}